{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few highlights\n",
    "\n",
    "# GPT-like (also called auto-regressive Transformer models)\n",
    "# BERT-like (also called auto-encoding Transformer models)\n",
    "# BART/T5-like (also called sequence-to-sequence Transformer models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Overview of the Model:**\n",
    "   - The model is composed of two main blocks: the Encoder (left) and the Decoder (right).\n",
    "   - **Encoder:** Receives input, builds a representation (features), and optimizes for understanding.\n",
    "   - **Decoder:** Uses the encoder's representation and other inputs to generate a target sequence, optimizing for output generation.\n",
    "\n",
    "### 2. **Transformer Architecture:**\n",
    "   - The model follows the Transformer architecture.\n",
    "   - Three types of models based on usage:\n",
    "      - Encoder-only models: Good for tasks like sentence classification.\n",
    "      - Decoder-only models: Good for generative tasks like text generation.\n",
    "      - Encoder-decoder models (sequence-to-sequence): Good for generative tasks like translation or summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Transformer Block](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. **Attention Layers:**\n",
    "   - Transformer models have special layers called attention layers.\n",
    "   - These layers enable the model to focus on specific words in a sentence, ignoring others during representation.\n",
    "   - Attention is crucial for tasks like translation, where the translation of a word depends on the context of surrounding words.\n",
    "\n",
    "### 4. **Original Transformer Architecture:**\n",
    "   - Initially designed for translation tasks.\n",
    "   - Encoder and decoder work together during training.\n",
    "   - Encoder attention layers use all words in a sentence.\n",
    "   - Decoder attention layers work sequentially, considering only previously translated words.\n",
    "   - Attention mask prevents the model from focusing on special words (e.g., padding) during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Transformer Block](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. **Architecture vs. Checkpoints vs. Models:**\n",
    "   - **Architecture:** The model's skeleton, defining layers and operations.\n",
    "   - **Checkpoints:** Weights that are loaded into a specific architecture.\n",
    "   - **Model:** An umbrella term that can refer to both architecture and checkpoints. This course distinguishes between architecture and checkpoint to reduce ambiguity.\n",
    "   - Example: BERT is an architecture, while bert-base-cased is a checkpoint for the first release of BERT.\n",
    "\n",
    "### 6. **Use Cases:**\n",
    "   - Different model configurations are suitable for different NLP tasks.\n",
    "   - Encoder-only models are good for understanding input.\n",
    "   - Decoder-only models are suitable for generative tasks.\n",
    "   - Encoder-decoder models are effective for generative tasks requiring input, like translation or summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Encoder-Only Models:**\n",
    "   - Encoder models utilize only the encoder block of a Transformer model.\n",
    "   - Attention layers in the encoder can access all words in the initial sentence, providing a \"bi-directional\" attention.\n",
    "   - Often referred to as auto-encoding models because they are designed for tasks involving the reconstruction or understanding of the input sentence.\n",
    "\n",
    "### 2. **Pretraining Process:**\n",
    "   - During pretraining, the models are trained by corrupting a given sentence (e.g., masking random words) and then tasking the model with reconstructing or finding the original sentence.\n",
    "   - The goal is to enable the model to capture a meaningful representation of the input sentence.\n",
    "\n",
    "### 3. **Suitability for Tasks:**\n",
    "   - Encoder models are well-suited for tasks that require understanding the entire sentence.\n",
    "   - Examples of tasks include sentence classification, named entity recognition, word classification, and extractive question answering.\n",
    "\n",
    "### 4. **Representative Encoder Models:**\n",
    "   - Several models fall under the category of encoder-only models. Some representatives mentioned in the text include:\n",
    "     - ALBERT\n",
    "     - BERT\n",
    "     - DistilBERT\n",
    "     - ELECTRA\n",
    "     - RoBERTa\n",
    "\n",
    "### 5. **Examples of Use Cases:**\n",
    "   - Sentence Classification: Determining the category or class of a given sentence.\n",
    "   - Named Entity Recognition: Identifying and classifying entities (e.g., names of people, organizations) in a sentence.\n",
    "   - Word Classification: Categorizing individual words based on their context.\n",
    "   - Extractive Question Answering: Identifying relevant portions of a text that answer a given question.\n",
    "\n",
    "### 6. **Bi-Directional Attention:**\n",
    "   - The ability of attention layers to consider both preceding and succeeding words in a sentence enhances the model's understanding of context.\n",
    "\n",
    "\n",
    "### 7. **Characteristics of Encoder Models:**\n",
    "   - These models excel in capturing contextual information and relationships between words in a sentence.\n",
    "   - The pretraining process focuses on learning meaningful representations from corrupted sentences.\n",
    "\n",
    "In summary, encoder-only models, such as BERT and its variants, are designed to understand and represent the full context of input sentences, making them suitable for various NLP tasks that require a comprehensive understanding of language structure and meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Decoder-Only Models:**\n",
    "   - Decoder models use only the decoder block of a Transformer model.\n",
    "   - Attention layers in the decoder can access only the words positioned before the current word in the sentence.\n",
    "   - These models are often referred to as auto-regressive models.\n",
    "\n",
    "### 2. **Auto-Regressive Nature:**\n",
    "   - The term \"auto-regressive\" implies that, at each stage, the model generates the next word in the sequence based on the preceding words it has generated.\n",
    "\n",
    "### 3. **Pretraining Process:**\n",
    "   - During pretraining, decoder models are typically trained by predicting the next word in a sentence.\n",
    "   - The goal is to enable the model to generate coherent and contextually relevant sequences of words.\n",
    "\n",
    "### 4. **Suitability for Tasks:**\n",
    "   - Decoder models are particularly well-suited for tasks involving text generation, where the model is tasked with creating meaningful and coherent text.\n",
    "\n",
    "### 5. **Representative Decoder Models:**\n",
    "   - Notable models in the category of decoder-only models include:\n",
    "     - CTRL\n",
    "     - GPT (Generative Pretrained Transformer)\n",
    "     - GPT-2\n",
    "     - Transformer XL\n",
    "\n",
    "### 6. **Text Generation Tasks:**\n",
    "   - Decoder models excel in tasks where the generation of human-like text is required.\n",
    "   - Examples include language modeling, story generation, and creative writing.\n",
    "\n",
    "### 7. **Attention Layer Limitations:**\n",
    "   - The attention layers in the decoder focus only on the preceding words, limiting the model's ability to consider future context during generation.\n",
    "\n",
    "### 8. **Text Generation Capabilities:**\n",
    "   - Decoder models are powerful tools for creative tasks that involve generating text based on learned patterns and context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Encoder-Decoder Models (Sequence-to-Sequence Models):**\n",
    "   - Utilize both the encoder and decoder blocks of the Transformer architecture.\n",
    "   - During each stage, the attention layers of the encoder can access all words in the initial sentence, while the attention layers of the decoder can only access words positioned before the current word in the input.\n",
    "\n",
    "### 2. **Pretraining Process:**\n",
    "   - Pretraining of these models can adopt objectives from both encoder and decoder models, but it often involves more complex tasks.\n",
    "   - For instance, T5 is pretrained by replacing random spans of text with a single mask special word, and the objective is then to predict the text replaced by this mask word.\n",
    "\n",
    "### 3. **Suitability for Tasks:**\n",
    "   - Sequence-to-sequence models are well-suited for tasks involving the generation of new sentences based on a given input.\n",
    "   - Examples of tasks include summarization, translation, and generative question answering.\n",
    "\n",
    "### 4. **Representative Models:**\n",
    "   - Notable models in the category of sequence-to-sequence models include:\n",
    "     - BART\n",
    "     - mBART\n",
    "     - Marian\n",
    "     - T5 (Text-To-Text Transfer Transformer)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
