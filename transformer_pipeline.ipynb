{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers \n",
    "\n",
    "# transformer library from hugging face\n",
    "# basic object --> pipeline() function : connects a model with its necessary preprocessing and post postprocessing steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.6276514530181885}]\n",
      "[{'label': 'POSITIVE', 'score': 0.6276514530181885}, {'label': 'NEGATIVE', 'score': 0.9888225793838501}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "classifier = pipeline('sentiment-analysis') \n",
    "result_1= classifier('I have been waiting to learn NLP course my whole life')\n",
    "result_2 = classifier(\n",
    "    ['I have been waiting to learn NLP course my whole life',\n",
    "    'I litle nervous']\n",
    "    )\n",
    "print(result_1)\n",
    "print(result_2)\n",
    "\n",
    "# model is downloaded and cached when you create the classifier object.\n",
    "# rerun the command, the cached model will be used instead\n",
    "\n",
    "# Some of the currently available pipelines are:\n",
    "# -----------------------------------------------\n",
    "# feature-extraction (get the vector representation of a text)\n",
    "# fill-mask\n",
    "# ner (named entity recognition)\n",
    "# question-answering\n",
    "# sentiment-analysis\n",
    "# summarization\n",
    "# text-generation\n",
    "# translation\n",
    "# zero-shot-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'this is a course about the Transformer library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.9569939970970154, 0.030598720535635948, 0.01240723580121994]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zero-shot classifiction \n",
    "classifier_zs = pipeline('zero-shot-classification') \n",
    "classifier_zs(\n",
    "    'this is a course about the Transformer library',\n",
    "    candidate_labels = ['education','politics', 'business']\n",
    ")\n",
    "# allows you to specify which labels to use for the classification.\n",
    "# This pipeline is called zero-shot because you don’t need to fine-tune the model on your data to use it. \n",
    "# It can directly return probability scores for any list of labels you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to play and write the first chapter of the \"Magic Magic of the Golden Ages\" (part 3, part 4, part 5, part 6, part 7, part 8, part 9, part 10'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text generation \n",
    "generator = pipeline('text-generation')\n",
    "generator('In this course, we will teach you how to')\n",
    "# you provide a prompt and the model will auto-complete it by generating the remaining text. \n",
    "# text generation involves randomness\n",
    "# it’s normal if you don’t get the same results as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to set the \"T\" into operation, and will show you how to set it in real time and within seconds. In this course we will show you how to set the \"T\" into operation,'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using a specfic model from hugging face hug, (instead of default model)\n",
    "generator_disgpt2 = pipeline('text-generation', model='distilgpt2')\n",
    "generator_disgpt2('In this course, we will teach you how to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19631513953208923,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models'},\n",
       " {'score': 0.04449228197336197,\n",
       "  'token': 745,\n",
       "  'token_str': ' building',\n",
       "  'sequence': 'This course will teach you all about building models'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask filling\n",
    "unmask = pipeline('fill-mask')\n",
    "unmask('This course will teach you all about <mask> models', top_k =2)\n",
    "# The top_k argument controls how many possibilities you want to be displayed. \n",
    "# Note that here the model fills in the special <mask> word, which is often referred to as a mask token. \n",
    "# Other mask-filling models might have different mask tokens, so it’s always good to verify the proper mask word when exploring other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "model.safetensors: 100%|██████████| 1.33G/1.33G [01:35<00:00, 14.0MB/s]\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "tokenizer_config.json: 100%|██████████| 60.0/60.0 [00:00<00:00, 591kB/s]\n",
      "vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 20.7MB/s]\n",
      "/home/sudouser/anaconda3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9993941,\n",
       "  'word': 'Praveen Veera',\n",
       "  'start': 11,\n",
       "  'end': 24},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.585764,\n",
       "  'word': 'NL',\n",
       "  'start': 43,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9987062,\n",
       "  'word': 'Hyderabad',\n",
       "  'start': 52,\n",
       "  'end': 61}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ner\n",
    "ner = pipeline('ner', grouped_entities = True)\n",
    "ner('My name is Praveen Veera and I am learning NLP from Hyderabad')\n",
    "# We pass the option grouped_entities=True in the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "model.safetensors: 100%|██████████| 261M/261M [00:17<00:00, 14.8MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 151kB/s]\n",
      "vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 13.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 13.0MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.5419266819953918, 'start': 39, 'end': 43, 'answer': 'ITCI'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qa\n",
    "qa = pipeline('question-answering')\n",
    "qa(\n",
    "    question = 'Where do I work?',\n",
    "    context='My name is Praveen Veera and I work at ITCI from Hyderabad'\n",
    "    \n",
    ")\n",
    "# Note that this pipeline works by extracting information from the provided context; it does not generate the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation\n",
    "translator = pipeline('translation', model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\") # French to English\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|██████████| 1.80k/1.80k [00:00<00:00, 1.96MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 1.22G/1.22G [01:36<00:00, 12.7MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 32.9kB/s]\n",
      "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 10.1MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 15.1MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Data Scientist with over 8.11 years of IT experience, specializing in data-centric applications, predictive modelling, Generative AI, and MLOps . Proficient in designing and implementing solutions using Azure AI Studio and Azure Machine Learning . Awarded Microsoft #BuildFor2030 Hackathon in 2022, focused on Climate Action and Sustainability .'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarization\n",
    "summerization = pipeline('summarization')\n",
    "summerization(\n",
    "    '''m a Data Scientist with over 8.11 years of IT experience, specializing in data-centric applications, predictive modelling, Generative AI, and MLOps. Proficient in designing and implementing solutions using Azure AI Studio and Azure Machine Learning.\n",
    "\n",
    "Accomplishments:\n",
    "* Consistently delivering adaptable machine learning solutions, rooted in strong MLOps principles.\n",
    "* Holder of the Microsoft Azure Data Scientist Associate certification.\n",
    "* Winner of the Microsoft #BuildFor2030 Hackathon in 2022, focused on Climate Action and Sustainability.\n",
    "* Enhanced my academic qualifications with a Post Graduate Diploma in Machine Learning and Artificial Intelligence.\n",
    "\n",
    "AI Leadership and Expertise:\n",
    "* Active contributor to ITC Infotech's Center of Excellence, specializing in Generative AI and Azure AI.\n",
    "* Proficient in identifying AI and ML opportunities for process optimization, automation, and decision enhancement.\n",
    "* Skilled in designing end-to-end AI and ML architectures aligned with business goals, considering data integration, model selection, and scalability.\n",
    "* Leading discussions on AI solution design, providing technical leadership and insights.\n",
    "* Evaluating and selecting AI technologies, frameworks, and tools for solution development.\n",
    "* Leading experiments and initiatives in Generative AI, overseeing multiple use cases and task management.\n",
    "* Actively participating in pioneering initiatives with expertise in OpenAI, Azure OpenAI, Azure Cognitive Services, and Langchain.\n",
    "* Proficient in developing Generative AI conversation chatbots and solving Retrieval-augmented Generation (RAG) based GenAI use cases.\n",
    "\n",
    "GenAI Use Cases:\n",
    "Led and contributed to diverse GenAI use cases, including:\n",
    "* Medical Product Comparison: Leveraged Generative AI for medical product comparisons through natural language interactions, extracting information from product documentation.\n",
    "* Automated Medical Paper Writing: Developed AI chatbots to expedite the creation of medical research papers by referencing previous results and current research data.\n",
    "* FAQ Generation for Medical Products: Designed an AI solution to auto-generate product FAQs from internal sources, reducing manual effort and turnaround time.\n",
    "* Multiple-choice questions (MCQs) for LnD Team: Created automated solutions for MCQ generation, facilitating training and educational content development.\n",
    "\n",
    "I'm passionate about leveraging AI to drive innovation, streamline processes, and enhance decision-making. I'm open to connecting for opportunities at the intersection of data-driven innovation and technology.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
